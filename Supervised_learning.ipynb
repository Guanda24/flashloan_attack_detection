{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d5f53f-40d0-4480-ae47-0c3d3471df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, precision_recall_curve, roc_curve\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b265e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3824159/1449706695.py:3: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  unlabeled = pd.read_csv('/local/scratch/exported/MP_Defi_txs_TY_23/guanda/unlabeled.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "attack_label = pd.read_csv('/local/scratch/exported/MP_Defi_txs_TY_23/guanda/attack_label.csv')\n",
    "unlabeled = pd.read_csv('/local/scratch/exported/MP_Defi_txs_TY_23/guanda/unlabeled.csv')\n",
    "\n",
    "# Select features\n",
    "features = ['from_address_profit', 'to_address_profit', 'highest_profit_in_usd',\n",
    "            'highest_price_change_ratio', 'path_length', 'num_swap_events', 'flashloan_in_usd']\n",
    "\n",
    "# Preprocessing\n",
    "unlabeled['flashloan_in_usd'] = pd.to_numeric(unlabeled['flashloan_in_usd'], errors='coerce')\n",
    "normal_label = unlabeled[unlabeled['highest_profit_in_usd'] <= 1000]\n",
    "unlabeled = unlabeled[unlabeled['highest_profit_in_usd'] > 1000].reset_index(drop=True)\n",
    "\n",
    "# Add a new column for labels: 1 for attack and 0 for normal\n",
    "attack_label['label'] = 1  # Attack\n",
    "normal_label['label'] = 0  # Normal\n",
    "\n",
    "# Combine the datasets\n",
    "combined_df = pd.concat([attack_label, normal_label], ignore_index=True)\n",
    "\n",
    "# Convert 'flashloan_in_usd' to numeric, coercing errors to NaN\n",
    "combined_df['flashloan_in_usd'] = pd.to_numeric(combined_df['flashloan_in_usd'], errors='coerce')\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df['label']\n",
    "X_unlabeled = unlabeled[features]\n",
    "\n",
    "# Combine X and X_unlabeled temporarily for imputation and scaling\n",
    "X_combined = np.vstack([X, X_unlabeled])\n",
    "\n",
    "# Handle missing values by imputing with the mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_combined_imputed = imputer.fit_transform(X_combined)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_combined_scaled = scaler.fit_transform(X_combined_imputed)\n",
    "\n",
    "# Separate them back into X and X_unlabeled\n",
    "X_scaled = X_combined_scaled[:len(X), :]\n",
    "X_unlabeled_scaled = X_combined_scaled[len(X):, :]\n",
    "\n",
    "temp = [\n",
    "\"0x3b19e152943f31fe0830b67315ddc89be9a066dc89174256e17bc8c2d35b5af8\",\n",
    "\"0xcb0ad9da33ecabf75df0a24aabf8a4517e4a7c5b1b2f11fee3b6a1ad9299a282\",\n",
    "\"0xcb58fb952914896b35d909136b9f719b71fc8bc60b59853459fc2476d4369c3a\",\n",
    "\"0xf72f1d10fc6923f87279ce6c0aef46e372c6652a696f280b0465a301a92f2e26\",\n",
    "\"0x118b7b7c11f9e9bd630ea84ef267b183b34021b667f4a3061f048207d266437a\",\n",
    "\"0x3503253131644dd9f52802d071de74e456570374d586ddd640159cf6fb9b8ad8\",\n",
    "\"0x35f8d2f572fceaac9288e5d462117850ef2694786992a8c3f6d02612277b0877\",\n",
    "\"0x0fc6d2ca064fc841bc9b1c1fad1fbb97bcea5c9a1b2b66ef837f1227e06519a6\",\n",
    "\"0x958236266991bc3fe3b77feaacea120f172c0708ad01c7a715b255f218f9313c\",\n",
    "\"0x46a03488247425f845e444b9c10b52ba3c14927c687d38287c0faddc7471150a\",\n",
    "\"0x8bb8dc5c7c830bac85fa48acad2505e9300a91c3ff239c9517d0cae33b595090\",\n",
    "\"0xf6022012b73770e7e2177129e648980a82aab555f9ac88b8a9cda3ec44b30779\",\n",
    "\"0xcd314668aaa9bbfebaf1a0bd2b6553d01dd58899c508d4729fa7311dc5d33ad7\"\n",
    "]\n",
    "\n",
    "indices_temp = combined_df[combined_df['tx_hash'].isin(temp)].index\n",
    "\n",
    "X_temp_scaled = X_scaled[indices_temp]\n",
    "y_temp = y.iloc[indices_temp]\n",
    "\n",
    "X_scaled_removed = np.delete(X_scaled, indices_temp, axis=0)\n",
    "y_array = np.array(y) \n",
    "y_removed = np.delete(y_array, indices_temp, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_removed, y_removed, test_size=0.2, random_state=42, stratify=y_removed)\n",
    "\n",
    "X_train = np.vstack([X_train, X_temp_scaled])\n",
    "y_train = np.concatenate([y_train, y_temp])\n",
    "\n",
    "unlabeled_predictions = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907af5c",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83543763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/gzhao/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/user/gzhao/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing Random Forest model.\n",
      "bootstrap: True\n",
      "ccp_alpha: 0.0\n",
      "class_weight: None\n",
      "criterion: gini\n",
      "max_depth: 18\n",
      "max_features: sqrt\n",
      "max_leaf_nodes: None\n",
      "max_samples: None\n",
      "min_impurity_decrease: 0.0\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 4\n",
      "min_weight_fraction_leaf: 0.0\n",
      "monotonic_cst: None\n",
      "n_estimators: 117\n",
      "n_jobs: None\n",
      "oob_score: False\n",
      "random_state: 42\n",
      "verbose: 0\n",
      "warm_start: False\n",
      "Number of 0s (normal): 174492\n",
      "Number of 1s (attack): 58175\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def rf_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                   min_samples_split=min_samples_split, random_state=42)\n",
    "    \n",
    "    # Use StratifiedKFold for stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=-1, scoring='f1').mean()\n",
    "    return score\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists('Models/random_forest_model.pkl'):\n",
    "    # Load the existing model\n",
    "    best_model = joblib.load('Models/random_forest_model.pkl')\n",
    "    print(\"Loaded existing Random Forest model.\")\n",
    "    best_params = best_model.get_params()\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "else: \n",
    "    # Optimize with Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(rf_objective, n_trials=50)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, 'Models/random_forest_model.pkl')\n",
    "    print(\"Trained and saved new Random Forest model.\")\n",
    "\n",
    "# Fit and evaluate\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate confusion matrix and AUC-ROC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame to store Precision-Recall data\n",
    "pr_data_df = pd.DataFrame({\n",
    "    'Recall': recall,\n",
    "    'Precision': precision\n",
    "})\n",
    "\n",
    "# Save the Precision-Recall data to a CSV file\n",
    "pr_data_df.to_csv('Models/random_forest_precision_recall_data.csv', index=False)\n",
    "\n",
    "# Prepare confusion matrix data\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_list = [\n",
    "    {'Metric': 'True Negatives', 'Value': tn},\n",
    "    {'Metric': 'False Positives', 'Value': fp},\n",
    "    {'Metric': 'False Negatives', 'Value': fn},\n",
    "    {'Metric': 'True Positives', 'Value': tp},\n",
    "    {'Metric': 'AUC-ROC', 'Value': auc_roc}\n",
    "]\n",
    "\n",
    "# Add classification report data to the metrics DataFrame\n",
    "for label, metrics in class_report.items():\n",
    "    if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        metrics_list.append({'Metric': f'Precision ({label})', 'Value': metrics['precision']})\n",
    "        metrics_list.append({'Metric': f'Recall ({label})', 'Value': metrics['recall']})\n",
    "        metrics_list.append({'Metric': f'F1-Score ({label})', 'Value': metrics['f1-score']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv('Models/random_forest_metrics.csv', index=False)\n",
    "\n",
    "# Calculate ROC curve data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame for ROC curve data\n",
    "roc_data_df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Save ROC curve data to CSV\n",
    "roc_data_df.to_csv('Models/random_forest_roc_data.csv', index=False)\n",
    "\n",
    "# Predict the labels for the unlabeled dataset\n",
    "rf_y_unlabeled_pred = best_model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Count the number of 0s and 1s in the predictions\n",
    "num_zeros = np.sum(rf_y_unlabeled_pred == 0)\n",
    "num_ones = np.sum(rf_y_unlabeled_pred == 1)\n",
    "\n",
    "print(f\"Number of 0s (normal): {num_zeros}\")\n",
    "print(f\"Number of 1s (attack): {num_ones}\")\n",
    "\n",
    "unlabeled['prediction'] = rf_y_unlabeled_pred\n",
    "platform_prediction_counts = unlabeled.groupby(['platform', 'prediction']).size().unstack(fill_value=0)\n",
    "platform_prediction_counts.to_csv('Models/random_forest_unlabeled_platform_prediction_counts.csv')\n",
    "\n",
    "unlabeled_predictions['Random_Forest'] = rf_y_unlabeled_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8e6c5",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8a9f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing K-Nearest Neighbors model.\n",
      "algorithm: auto\n",
      "leaf_size: 30\n",
      "metric: minkowski\n",
      "metric_params: None\n",
      "n_jobs: None\n",
      "n_neighbors: 5\n",
      "p: 2\n",
      "weights: distance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/gzhao/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s (normal): 206391\n",
      "Number of 1s (attack): 26276\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def knn_objective(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "    \n",
    "    # Use StratifiedKFold for stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=-1, scoring='f1').mean()\n",
    "    return score\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists('Models/knn_model.pkl'):\n",
    "    # Load the existing model\n",
    "    best_model = joblib.load('Models/knn_model.pkl')\n",
    "    print(\"Loaded existing K-Nearest Neighbors model.\")\n",
    "    best_params = best_model.get_params()\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "else: \n",
    "    # Optimize with Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(knn_objective, n_trials=50)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, 'Models/knn_model.pkl')\n",
    "    print(\"Trained and saved new K-Nearest Neighbors model.\")\n",
    "\n",
    "# Fit and evaluate\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate confusion matrix and AUC-ROC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame to store Precision-Recall data\n",
    "pr_data_df = pd.DataFrame({\n",
    "    'Recall': recall,\n",
    "    'Precision': precision\n",
    "})\n",
    "\n",
    "# Save the Precision-Recall data to a CSV file\n",
    "pr_data_df.to_csv('Models/knn_precision_recall_data.csv', index=False)\n",
    "\n",
    "# Prepare confusion matrix data\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_list = [\n",
    "    {'Metric': 'True Negatives', 'Value': tn},\n",
    "    {'Metric': 'False Positives', 'Value': fp},\n",
    "    {'Metric': 'False Negatives', 'Value': fn},\n",
    "    {'Metric': 'True Positives', 'Value': tp},\n",
    "    {'Metric': 'AUC-ROC', 'Value': auc_roc}\n",
    "]\n",
    "\n",
    "# Add classification report data to the metrics DataFrame\n",
    "for label, metrics in class_report.items():\n",
    "    if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        metrics_list.append({'Metric': f'Precision ({label})', 'Value': metrics['precision']})\n",
    "        metrics_list.append({'Metric': f'Recall ({label})', 'Value': metrics['recall']})\n",
    "        metrics_list.append({'Metric': f'F1-Score ({label})', 'Value': metrics['f1-score']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv('Models/knn_metrics.csv', index=False)\n",
    "\n",
    "# Calculate ROC curve data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame for ROC curve data\n",
    "roc_data_df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "\n",
    "# Save ROC curve data to CSV\n",
    "roc_data_df.to_csv('Models/knn_roc_data.csv', index=False)\n",
    "\n",
    "# Predict the labels for the unlabeled dataset\n",
    "knn_y_unlabeled_pred = best_model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Count the number of 0s and 1s in the predictions\n",
    "num_zeros = np.sum(knn_y_unlabeled_pred == 0)\n",
    "num_ones = np.sum(knn_y_unlabeled_pred == 1)\n",
    "\n",
    "print(f\"Number of 0s (normal): {num_zeros}\")\n",
    "print(f\"Number of 1s (attack): {num_ones}\")\n",
    "\n",
    "unlabeled['prediction'] = knn_y_unlabeled_pred\n",
    "platform_prediction_counts = unlabeled.groupby(['platform', 'prediction']).size().unstack(fill_value=0)\n",
    "platform_prediction_counts.to_csv('Models/knn_unlabeled_platform_prediction_counts.csv')\n",
    "\n",
    "unlabeled_predictions['KNN'] = knn_y_unlabeled_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320a541",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b96102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing Decision Tree model.\n",
      "ccp_alpha: 0.0\n",
      "class_weight: None\n",
      "criterion: gini\n",
      "max_depth: 13\n",
      "max_features: None\n",
      "max_leaf_nodes: None\n",
      "min_impurity_decrease: 0.0\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 9\n",
      "min_weight_fraction_leaf: 0.0\n",
      "monotonic_cst: None\n",
      "random_state: 42\n",
      "splitter: best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/gzhao/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s (normal): 177768\n",
      "Number of 1s (attack): 54899\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def dt_objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
    "    \n",
    "    # Use StratifiedKFold for stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=-1, scoring='f1').mean()\n",
    "    return score\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists('Models/decision_tree_model.pkl'):\n",
    "    # Load the existing model\n",
    "    best_model = joblib.load('Models/decision_tree_model.pkl')\n",
    "    print(\"Loaded existing Decision Tree model.\")\n",
    "    best_params = best_model.get_params()\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "else: \n",
    "    # Optimize with Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(dt_objective, n_trials=50)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, 'Models/decision_tree_model.pkl')\n",
    "    print(\"Trained and saved new Decision Tree model.\")\n",
    "\n",
    "# Fit and evaluate\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate confusion matrix and AUC-ROC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame to store Precision-Recall data\n",
    "pr_data_df = pd.DataFrame({\n",
    "    'Recall': recall,\n",
    "    'Precision': precision\n",
    "})\n",
    "\n",
    "# Save the Precision-Recall data to a CSV file\n",
    "pr_data_df.to_csv('Models/decision_tree_precision_recall_data.csv', index=False)\n",
    "\n",
    "# Prepare confusion matrix data\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_list = [\n",
    "    {'Metric': 'True Negatives', 'Value': tn},\n",
    "    {'Metric': 'False Positives', 'Value': fp},\n",
    "    {'Metric': 'False Negatives', 'Value': fn},\n",
    "    {'Metric': 'True Positives', 'Value': tp},\n",
    "    {'Metric': 'AUC-ROC', 'Value': auc_roc}\n",
    "]\n",
    "\n",
    "# Add classification report data to the metrics DataFrame\n",
    "for label, metrics in class_report.items():\n",
    "    if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        metrics_list.append({'Metric': f'Precision ({label})', 'Value': metrics['precision']})\n",
    "        metrics_list.append({'Metric': f'Recall ({label})', 'Value': metrics['recall']})\n",
    "        metrics_list.append({'Metric': f'F1-Score ({label})', 'Value': metrics['f1-score']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv('Models/decision_tree_metrics.csv', index=False)\n",
    "\n",
    "# Calculate ROC curve data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame for ROC curve data\n",
    "roc_data_df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "\n",
    "# Save ROC curve data to CSV\n",
    "roc_data_df.to_csv('Models/decision_tree_roc_data.csv', index=False)\n",
    "\n",
    "# Predict the labels for the unlabeled dataset\n",
    "dt_y_unlabeled_pred = best_model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Count the number of 0s and 1s in the predictions\n",
    "num_zeros = np.sum(dt_y_unlabeled_pred == 0)\n",
    "num_ones = np.sum(dt_y_unlabeled_pred == 1)\n",
    "\n",
    "print(f\"Number of 0s (normal): {num_zeros}\")\n",
    "print(f\"Number of 1s (attack): {num_ones}\")\n",
    "\n",
    "unlabeled['prediction'] = dt_y_unlabeled_pred\n",
    "platform_prediction_counts = unlabeled.groupby(['platform', 'prediction']).size().unstack(fill_value=0)\n",
    "platform_prediction_counts.to_csv('Models/decision_tree_unlabeled_platform_prediction_counts.csv')\n",
    "\n",
    "unlabeled_predictions['Decision_Tree'] = dt_y_unlabeled_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdad9c",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec17a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing Gradient Boosting model.\n",
      "ccp_alpha: 0.0\n",
      "criterion: friedman_mse\n",
      "init: None\n",
      "learning_rate: 0.1\n",
      "loss: log_loss\n",
      "max_depth: 8\n",
      "max_features: None\n",
      "max_leaf_nodes: None\n",
      "min_impurity_decrease: 0.0\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 2\n",
      "min_weight_fraction_leaf: 0.0\n",
      "n_estimators: 92\n",
      "n_iter_no_change: None\n",
      "random_state: 42\n",
      "subsample: 1.0\n",
      "tol: 0.0001\n",
      "validation_fraction: 0.1\n",
      "verbose: 0\n",
      "warm_start: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/gzhao/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingClassifier from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s (normal): 171611\n",
      "Number of 1s (attack): 61056\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def gb_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # Use StratifiedKFold for stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=-1, scoring='f1').mean()\n",
    "    return score\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists('Models/gradient_boosting_model.pkl'):\n",
    "    # Load the existing model\n",
    "    best_model = joblib.load('Models/gradient_boosting_model.pkl')\n",
    "    print(\"Loaded existing Gradient Boosting model.\")\n",
    "    best_params = best_model.get_params()\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "else: \n",
    "    # Optimize with Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(gb_objective, n_trials=50)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, 'Models/gradient_boosting_model.pkl')\n",
    "    print(\"Trained and saved new Gradient Boosting model.\")\n",
    "\n",
    "# Fit and evaluate\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate confusion matrix and AUC-ROC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame to store Precision-Recall data\n",
    "pr_data_df = pd.DataFrame({\n",
    "    'Recall': recall,\n",
    "    'Precision': precision\n",
    "})\n",
    "\n",
    "# Save the Precision-Recall data to a CSV file\n",
    "pr_data_df.to_csv('Models/gradient_boosting_precision_recall_data.csv', index=False)\n",
    "\n",
    "# Prepare confusion matrix data\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_list = [\n",
    "    {'Metric': 'True Negatives', 'Value': tn},\n",
    "    {'Metric': 'False Positives', 'Value': fp},\n",
    "    {'Metric': 'False Negatives', 'Value': fn},\n",
    "    {'Metric': 'True Positives', 'Value': tp},\n",
    "    {'Metric': 'AUC-ROC', 'Value': auc_roc}\n",
    "]\n",
    "\n",
    "# Add classification report data to the metrics DataFrame\n",
    "for label, metrics in class_report.items():\n",
    "    if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        metrics_list.append({'Metric': f'Precision ({label})', 'Value': metrics['precision']})\n",
    "        metrics_list.append({'Metric': f'Recall ({label})', 'Value': metrics['recall']})\n",
    "        metrics_list.append({'Metric': f'F1-Score ({label})', 'Value': metrics['f1-score']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv('Models/gradient_boosting_metrics.csv', index=False)\n",
    "\n",
    "# Calculate ROC curve data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame for ROC curve data\n",
    "roc_data_df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "\n",
    "# Save ROC curve data to CSV\n",
    "roc_data_df.to_csv('Models/gradient_boosting_roc_data.csv', index=False)\n",
    "\n",
    "# Predict the labels for the unlabeled dataset\n",
    "gb_y_unlabeled_pred = best_model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Count the number of 0s and 1s in the predictions\n",
    "num_zeros = np.sum(gb_y_unlabeled_pred == 0)\n",
    "num_ones = np.sum(gb_y_unlabeled_pred == 1)\n",
    "\n",
    "print(f\"Number of 0s (normal): {num_zeros}\")\n",
    "print(f\"Number of 1s (attack): {num_ones}\")\n",
    "\n",
    "unlabeled['prediction'] = gb_y_unlabeled_pred\n",
    "platform_prediction_counts = unlabeled.groupby(['platform', 'prediction']).size().unstack(fill_value=0)\n",
    "platform_prediction_counts.to_csv('Models/gradient_boosting_unlabeled_platform_prediction_counts.csv')\n",
    "\n",
    "unlabeled_predictions['Gradient_Boosting'] = gb_y_unlabeled_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03acfcbb",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1218dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing AdaBoost model.\n",
      "algorithm: SAMME\n",
      "estimator: None\n",
      "learning_rate: 1.0\n",
      "n_estimators: 376\n",
      "random_state: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/gzhao/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator AdaBoostClassifier from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s (normal): 197214\n",
      "Number of 1s (attack): 35453\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def ab_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 400)\n",
    "    model = AdaBoostClassifier(n_estimators=n_estimators, algorithm='SAMME', random_state=42)\n",
    "    \n",
    "    # Use StratifiedKFold for stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=-1, scoring='f1').mean()\n",
    "    return score\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists('Models/adaboost_model.pkl'):\n",
    "    # Load the existing model\n",
    "    best_model = joblib.load('Models/adaboost_model.pkl')\n",
    "    print(\"Loaded existing AdaBoost model.\")\n",
    "    best_params = best_model.get_params()\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "else: \n",
    "    # Optimize with Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(ab_objective, n_trials=50)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = AdaBoostClassifier(**best_params, algorithm='SAMME', random_state=42)\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, 'Models/adaboost_model.pkl')\n",
    "    print(\"Trained and saved new AdaBoost model.\")\n",
    "\n",
    "# Fit and evaluate\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate confusion matrix and AUC-ROC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame to store Precision-Recall data\n",
    "pr_data_df = pd.DataFrame({\n",
    "    'Recall': recall,\n",
    "    'Precision': precision\n",
    "})\n",
    "\n",
    "# Save the Precision-Recall data to a CSV file\n",
    "pr_data_df.to_csv('Models/adaboost_precision_recall_data.csv', index=False)\n",
    "\n",
    "# Prepare confusion matrix data\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_list = [\n",
    "    {'Metric': 'True Negatives', 'Value': tn},\n",
    "    {'Metric': 'False Positives', 'Value': fp},\n",
    "    {'Metric': 'False Negatives', 'Value': fn},\n",
    "    {'Metric': 'True Positives', 'Value': tp},\n",
    "    {'Metric': 'AUC-ROC', 'Value': auc_roc}\n",
    "]\n",
    "\n",
    "# Add classification report data to the metrics DataFrame\n",
    "for label, metrics in class_report.items():\n",
    "    if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        metrics_list.append({'Metric': f'Precision ({label})', 'Value': metrics['precision']})\n",
    "        metrics_list.append({'Metric': f'Recall ({label})', 'Value': metrics['recall']})\n",
    "        metrics_list.append({'Metric': f'F1-Score ({label})', 'Value': metrics['f1-score']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv('Models/adaboost_metrics.csv', index=False)\n",
    "\n",
    "# Calculate ROC curve data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame for ROC curve data\n",
    "roc_data_df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "\n",
    "# Save ROC curve data to CSV\n",
    "roc_data_df.to_csv('Models/adaboost_roc_data.csv', index=False)\n",
    "\n",
    "# Predict the labels for the unlabeled dataset\n",
    "ada_y_unlabeled_pred = best_model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Count the number of 0s and 1s in the predictions\n",
    "num_zeros = np.sum(ada_y_unlabeled_pred == 0)\n",
    "num_ones = np.sum(ada_y_unlabeled_pred == 1)\n",
    "\n",
    "print(f\"Number of 0s (normal): {num_zeros}\")\n",
    "print(f\"Number of 1s (attack): {num_ones}\")\n",
    "\n",
    "unlabeled['prediction'] = ada_y_unlabeled_pred\n",
    "platform_prediction_counts = unlabeled.groupby(['platform', 'prediction']).size().unstack(fill_value=0)\n",
    "platform_prediction_counts.to_csv('Models/adaboost_unlabeled_platform_prediction_counts.csv')\n",
    "\n",
    "unlabeled_predictions['AdaBoost'] = ada_y_unlabeled_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6300a4a",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e225dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing XGBoost model.\n",
      "objective: binary:logistic\n",
      "base_score: None\n",
      "booster: None\n",
      "callbacks: None\n",
      "colsample_bylevel: None\n",
      "colsample_bynode: None\n",
      "colsample_bytree: None\n",
      "device: None\n",
      "early_stopping_rounds: None\n",
      "enable_categorical: False\n",
      "eval_metric: logloss\n",
      "feature_types: None\n",
      "gamma: None\n",
      "grow_policy: None\n",
      "importance_type: None\n",
      "interaction_constraints: None\n",
      "learning_rate: None\n",
      "max_bin: None\n",
      "max_cat_threshold: None\n",
      "max_cat_to_onehot: None\n",
      "max_delta_step: None\n",
      "max_depth: 12\n",
      "max_leaves: None\n",
      "min_child_weight: None\n",
      "missing: nan\n",
      "monotone_constraints: None\n",
      "multi_strategy: None\n",
      "n_estimators: 24\n",
      "n_jobs: None\n",
      "num_parallel_tree: None\n",
      "random_state: 42\n",
      "reg_alpha: None\n",
      "reg_lambda: None\n",
      "sampling_method: None\n",
      "scale_pos_weight: None\n",
      "subsample: None\n",
      "tree_method: None\n",
      "validate_parameters: None\n",
      "verbosity: None\n",
      "Number of 0s (normal): 160965\n",
      "Number of 1s (attack): 71702\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def xgb_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "    \n",
    "    model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, eval_metric='logloss', random_state=42)\n",
    "    \n",
    "    # Use StratifiedKFold for stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=-1, scoring='f1').mean()\n",
    "    return score\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists('Models/xgboost_model.pkl'):\n",
    "    # Load the existing model\n",
    "    best_model = joblib.load('Models/xgboost_model.pkl')\n",
    "    print(\"Loaded existing XGBoost model.\")\n",
    "    best_params = best_model.get_params()\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "else: \n",
    "    # Optimize with Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(xgb_objective, n_trials=50)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    best_model = XGBClassifier(**best_params, eval_metric='logloss', random_state=42)\n",
    "    # Save the model\n",
    "    joblib.dump(best_model, 'Models/xgboost_model.pkl')\n",
    "    print(\"Trained and saved new XGBoost model.\")\n",
    "\n",
    "# Fit and evaluate\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate confusion matrix and AUC-ROC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame to store Precision-Recall data\n",
    "pr_data_df = pd.DataFrame({\n",
    "    'Recall': recall,\n",
    "    'Precision': precision\n",
    "})\n",
    "\n",
    "# Save the Precision-Recall data to a CSV file\n",
    "pr_data_df.to_csv('Models/xgboost_precision_recall_data.csv', index=False)\n",
    "\n",
    "# Prepare confusion matrix data\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_list = [\n",
    "    {'Metric': 'True Negatives', 'Value': tn},\n",
    "    {'Metric': 'False Positives', 'Value': fp},\n",
    "    {'Metric': 'False Negatives', 'Value': fn},\n",
    "    {'Metric': 'True Positives', 'Value': tp},\n",
    "    {'Metric': 'AUC-ROC', 'Value': auc_roc}\n",
    "]\n",
    "\n",
    "# Add classification report data to the metrics DataFrame\n",
    "for label, metrics in class_report.items():\n",
    "    if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        metrics_list.append({'Metric': f'Precision ({label})', 'Value': metrics['precision']})\n",
    "        metrics_list.append({'Metric': f'Recall ({label})', 'Value': metrics['recall']})\n",
    "        metrics_list.append({'Metric': f'F1-Score ({label})', 'Value': metrics['f1-score']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv('Models/xgboost_metrics.csv', index=False)\n",
    "\n",
    "# Calculate ROC curve data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create a DataFrame for ROC curve data\n",
    "roc_data_df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "\n",
    "# Save ROC curve data to CSV\n",
    "roc_data_df.to_csv('Models/xgboost_roc_data.csv', index=False)\n",
    "\n",
    "# Predict the labels for the unlabeled dataset\n",
    "xg_y_unlabeled_pred = best_model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Count the number of 0s and 1s in the predictions\n",
    "num_zeros = np.sum(xg_y_unlabeled_pred == 0)\n",
    "num_ones = np.sum(xg_y_unlabeled_pred == 1)\n",
    "\n",
    "print(f\"Number of 0s (normal): {num_zeros}\")\n",
    "print(f\"Number of 1s (attack): {num_ones}\")\n",
    "\n",
    "unlabeled['prediction'] = xg_y_unlabeled_pred\n",
    "platform_prediction_counts = unlabeled.groupby(['platform', 'prediction']).size().unstack(fill_value=0)\n",
    "platform_prediction_counts.to_csv('Models/xgboost_unlabeled_platform_prediction_counts.csv')\n",
    "\n",
    "unlabeled_predictions['XGBoost'] = xg_y_unlabeled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b78b5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled['Random_Forest'] = unlabeled_predictions['Random_Forest']\n",
    "unlabeled['KNN'] = unlabeled_predictions['KNN']\n",
    "unlabeled['Decision_Tree'] = unlabeled_predictions['Decision_Tree']\n",
    "unlabeled['Gradient_Boosting'] = unlabeled_predictions['Gradient_Boosting']\n",
    "unlabeled['AdaBoost'] = unlabeled_predictions['AdaBoost']\n",
    "unlabeled['XGBoost'] = unlabeled_predictions['XGBoost']\n",
    "unlabeled_predictions = unlabeled_predictions.astype(bool)\n",
    "unlabeled['All_Attack'] = unlabeled_predictions.sum(axis=1) == len(unlabeled_predictions.columns)\n",
    "unlabeled.to_csv('Models/unlabeled_all_models.csv', index=False)\n",
    "all_attack_indices = unlabeled[unlabeled['All_Attack']].index\n",
    "all_attack_data = unlabeled.loc[all_attack_indices]\n",
    "all_attack_data.to_csv('Models/unlabeled_all_attack_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
